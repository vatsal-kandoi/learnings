{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG from scratch\n",
    "\n",
    "### References\n",
    "\n",
    "* https://towardsdatascience.com/local-rag-from-scratch-3afc6d3dea08\n",
    "* https://iamajithkumar.medium.com/how-to-use-chroma-to-build-your-first-similarity-search-5c054bfd5add\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_name = \"BAAI/bge-small-en-v1.5\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.save_pretrained(\"model/tokenizer\")\n",
    "model.save_pretrained(\"model/embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursivelyChunkTillOptimumSize(\n",
    "        chunks, \n",
    "        tokenizer, \n",
    "        chunk_size = 1024, \n",
    "        separator = None, \n",
    "        secondary_chunking_regex = None ):\n",
    "\n",
    "    modifiedChunks = []\n",
    "    refined = []\n",
    "    current_chunk = \"\"\n",
    "    # Split all words of size greater than chunk size into smaller chunks\n",
    "    while len(chunks) > 0:\n",
    "        chunk = chunks.pop(0)\n",
    "        chunk = chunk.strip()\n",
    "\n",
    "        if len(tokenizer.tokenize(chunk)) > chunk_size:\n",
    "            sub_chunks = re.split(secondary_chunking_regex, current_chunk)\n",
    "            chunks = sub_chunks + chunks\n",
    "        else:\n",
    "            modifiedChunks.append(chunk)\n",
    "\n",
    "    # Join small chunks ot make big chunks\n",
    "    current_chunk = \"\"\n",
    "    for chunk in modifiedChunks:\n",
    "        new_chunk = current_chunk + (separator if current_chunk else '') + chunk\n",
    "\n",
    "        if len(tokenizer.tokenize(new_chunk)) <= chunk_size:\n",
    "            current_chunk = new_chunk\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                refined.append(current_chunk)\n",
    "            current_chunk = chunk\n",
    "\n",
    "    if current_chunk:\n",
    "        refined.append(current_chunk)\n",
    "\n",
    "    return refined    \n",
    "    \n",
    "def chunk( \n",
    "        text, \n",
    "        tokenizer, \n",
    "        paragraph_separator = \"\\n\\n\", \n",
    "        chunk_size = 1024, \n",
    "        separator = \" \", \n",
    "        secondary_chunking_regex = r'\\S+?[\\.,;!?]',\n",
    "        chunk_overlap = 0 ):\n",
    "    # Divide documents into paragraphs\n",
    "    paragraphs = re.split(paragraph_separator, text)\n",
    "    all_chunks = []\n",
    "\n",
    "    for paragraph in paragraphs:\n",
    "        words = paragraph.split(separator)\n",
    "        chunks = recursivelyChunkTillOptimumSize(\n",
    "            words, \n",
    "            tokenizer, \n",
    "            chunk_size = chunk_size, \n",
    "            separator = separator, \n",
    "            secondary_chunking_regex = secondary_chunking_regex \n",
    "        )\n",
    "        all_chunks.extend(chunks)\n",
    "\n",
    "    if chunk_overlap == 0:\n",
    "        return all_chunks\n",
    "\n",
    "    final_chunks = []\n",
    "    for idx, chunk in enumerate(all_chunks):\n",
    "        if idx == 0:\n",
    "            final_chunks.append(chunk)\n",
    "            continue\n",
    "        # split the overlap between end of last chunk and start of next chunk\n",
    "        overlap_chunk = all_chunks[idx - 1][overlap_chunk // 2: ] + all_chunks[idx][0: overlap_chunk // 2]\n",
    "        final_chunks.extend( [overlap_chunk, chunk] )\n",
    "\n",
    "    return final_chunks    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing\n",
    "\n",
    "* Statisticals -> bag of words, sparse locations, etc\n",
    "* Machine learned -> embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(chunk, tokenizer, model):\n",
    "    inputs = tokenizer(chunk, return_tensors=\"pt\", padding=True, truncation=True) \n",
    "    \n",
    "    # Generate the embeddings \n",
    "    with torch.no_grad():    \n",
    "        embeddings = model(**inputs).last_hidden_state.mean(dim=1).squeeze()\n",
    "\n",
    "    return embeddings.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "chroma_client = chromadb.PersistentClient(path=\"vector_store\")\n",
    "collection = chroma_client.create_collection(name=\"embeddings\", metadata={\"hnsw:space\": \"cosine\"}, get_or_create = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Traditional sleigh bed crafted in rich walnut wood, featuring a curved headboard and footboard with intricate grain details. Queen size, includes a plush, supportive mattress. Produced by Heritage Bed Co. Dimensions: 65\\\"W x 85\\\"L x 50\\\"H.\"\n",
    "chunks = chunk(text, tokenizer, chunk_overlap = 512)\n",
    "for created_chunk in chunks:\n",
    "    embeddings = compute_embeddings(created_chunk, tokenizer, model)\n",
    "    collection.add(str(uuid.uuid1()), embeddings, {\"sku\": 1}, created_chunk )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 100 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    }
   ],
   "source": [
    "text = \"Size of the Traditional sleigh bed with walnut wood\"\n",
    "chunks = chunk(text, tokenizer, chunk_overlap = 512)\n",
    "embeddings = []\n",
    "\n",
    "for created_chunk in chunks:\n",
    "    embedding = compute_embeddings(created_chunk, tokenizer, model)\n",
    "    embeddings.append(embedding)\n",
    "\n",
    "res = collection.query(\n",
    "    query_embeddings=embeddings,\n",
    "    n_results=100,\n",
    "    include=['distances', 'documents', 'metadatas']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForQuestionAnswering\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "qa_tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "qa_model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateResponse(question, context, tokenizer, model):\n",
    "    print(question)\n",
    "    print(context)\n",
    "    input_ids = tokenizer.encode(question, context)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    sep_idx = tokens.index('[SEP]')\n",
    "    token_type_ids = [0 for i in range(sep_idx+1)] + [1 for i in range(sep_idx+1,len(tokens))]\n",
    "\n",
    "    out = model(torch.tensor([input_ids]), # The tokens representing our input text.\n",
    "                token_type_ids=torch.tensor([token_type_ids]))\n",
    "\n",
    "    start_logits,end_logits = out['start_logits'],out['end_logits']\n",
    "    answer_start = torch.argmax(start_logits)\n",
    "    answer_end = torch.argmax(end_logits)\n",
    "    ans = ''.join(tokens[answer_start:answer_end])\n",
    "\n",
    "    return ans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the dimensions of the walnut sleigh\n",
      "Traditional sleigh bed crafted in rich walnut wood, featuring a curved headboard and footboard with intricate grain details. Queen size, includes a plush, supportive mattress. Produced by Heritage Bed Co. Dimensions: 65\"W x 85\"L x 50\"H.\n",
      "65\"wx85\"lx50\"\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    generateResponse(\n",
    "        \"What is the dimensions of the walnut sleigh\", \n",
    "        \"\".join(res['documents'][0]),\n",
    "        qa_tokenizer,\n",
    "        qa_model\n",
    "    )\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
